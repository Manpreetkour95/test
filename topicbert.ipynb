{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee08396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d222087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d88134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf61e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love playing sports and staying active.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The economy is experiencing a downturn.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The latest movie is a must-watch for all film ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I enjoy reading books and learning new things.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0          I love playing sports and staying active.\n",
       "1            The economy is experiencing a downturn.\n",
       "2  The latest movie is a must-watch for all film ...\n",
       "3     I enjoy reading books and learning new things."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    \"I love playing sports and staying active.\",\n",
    "    \"The economy is experiencing a downturn.\",\n",
    "    \"The latest movie is a must-watch for all film enthusiasts.\",\n",
    "    \"I enjoy reading books and learning new things.\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\"text\": texts})\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27050f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "455eed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py:1595: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py:1595: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[1;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[0;32m   2867\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2868\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2869\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2687\u001b[0m                 \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# JH why raw data?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[1;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[0;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2739\u001b[1;33m             \u001b[0mtqdm_kwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm_kwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2740\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[1;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m             \u001b[0mmetric_kwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_kwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\spectral.py\u001b[0m in \u001b[0;36mspectral_layout\u001b[1;34m(data, graph, dim, random_state, metric, metric_kwds)\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mv0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[1;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[0;32m   1597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m             raise TypeError(\"Cannot use scipy.linalg.eigh for sparse A with \"\n\u001b[0m\u001b[0;32m   1599\u001b[0m                             \u001b[1;34m\"k >= N. Use scipy.linalg.eigh(A.toarray()) or\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23696\\1569944274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtopic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed_topic_list\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_guided_topic_modeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mumap_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reduce_dimensionality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;31m# Cluster reduced embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[1;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[0;32m   2870\u001b[0m                 logger.info(\"The dimensionality reduction algorithm did not contain the `y` parameter and\"\n\u001b[0;32m   2871\u001b[0m                             \" therefore the `y` parameter was not used\")\n\u001b[1;32m-> 2872\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2874\u001b[0m         \u001b[0mumap_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m                 \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# JH why raw data?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m             )\n\u001b[0;32m   2690\u001b[0m             \u001b[1;31m# Assign any points that are fully disconnected from our manifold(s) to have embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[1;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[0;32m   2737\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2739\u001b[1;33m             \u001b[0mtqdm_kwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm_kwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2740\u001b[0m         )\n\u001b[0;32m   2741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[1;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m             \u001b[0mmetric_kwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_kwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m         \u001b[0mexpansion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitialisation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\umap\\spectral.py\u001b[0m in \u001b[0;36mspectral_layout\u001b[1;34m(data, graph, dim, random_state, metric, metric_kwds)\u001b[0m\n\u001b[0;32m    337\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mv0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m             )\n\u001b[0;32m    341\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[1;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[0;32m   1596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m             raise TypeError(\"Cannot use scipy.linalg.eigh for sparse A with \"\n\u001b[0m\u001b[0;32m   1599\u001b[0m                             \u001b[1;34m\"k >= N. Use scipy.linalg.eigh(A.toarray()) or\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m                             \" reduce k.\")\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k."
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create a BERTopic instance\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the BERTopic model\n",
    "topics, _ = topic_model.fit_transform(data[\"text\"])\n",
    "\n",
    "# Get the dominant topics and their associated keywords\n",
    "topics_keywords = topic_model.get_topic_info()\n",
    "\n",
    "# Print the topics and their keywords\n",
    "for topic_id, topic_info in topics_keywords.iterrows():\n",
    "    print(f\"Topic {topic_id}: {topic_info['Keywords']}\")\n",
    "\n",
    "# Assign topics to documents\n",
    "topics_assigned = topic_model.transform(data[\"text\"])\n",
    "\n",
    "# Print the topics assigned to each document\n",
    "for doc_id, topic_id in enumerate(topics_assigned):\n",
    "    print(f\"Document {doc_id}: Topic {topic_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcf7d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "#topic_model = BERTopic()\n",
    "#topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c42639a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23696\\3213309497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[0;32m    344\u001b[0m             embeddings = self._extract_embeddings(documents.Document,\n\u001b[0;32m    345\u001b[0m                                                   \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"document\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                                                   verbose=self.verbose)\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Transformed documents to Embeddings\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m_extract_embeddings\u001b[1;34m(self, documents, method, verbose)\u001b[0m\n\u001b[0;32m   2826\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"document\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m             \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m             raise ValueError(\"Wrong method for extracting document/word embeddings. \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\bertopic\\backend\\_base.py\u001b[0m in \u001b[0;36membed_documents\u001b[1;34m(self, document, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \"\"\"\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"\"\"\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                 \u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'token_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0moutput_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m         )\n\u001b[0;32m   1031\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m                 )\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m         )\n\u001b[0;32m    502\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         )\n\u001b[0;32m    434\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1816\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1818\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1819\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "topic_model.fit_transform(docs)(docs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09564566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|███████████████████████████████████████████| 232k/232k [00:00<00:00, 676kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 5.60kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████| 570/570 [00:00<00:00, 114kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████████████████████████████████████| 440M/440M [00:11<00:00, 39.0MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.7631610631942749\n",
      "Epoch 2 - Train Loss: 0.6757266521453857\n",
      "Epoch 3 - Train Loss: 0.6390253305435181\n",
      "Epoch 4 - Train Loss: 0.5525497198104858\n",
      "Epoch 5 - Train Loss: 0.4450497627258301\n",
      "Validation Accuracy: 0.6\n",
      "Test Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "# Step 1: Dataset Preparation\n",
    "\n",
    "train_data = {\n",
    "    'text': [\n",
    "        \"I loved the movie. It was amazing!\",\n",
    "        \"The acting was superb.\",\n",
    "        \"The plot twist at the end caught me by surprise.\",\n",
    "        \"I would highly recommend this film.\",\n",
    "        \"The movie was a bit slow in the beginning, but it picked up later.\"\n",
    "    ],\n",
    "    'labels': [1, 1, 1, 1, 0]  # 1 represents positive sentiment, 0 represents negative sentiment\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'text': [\n",
    "        \"The cinematography was excellent.\",\n",
    "        \"I didn't enjoy the movie. It was boring.\",\n",
    "        \"The characters were well-developed.\",\n",
    "        \"The story lacked depth.\",\n",
    "        \"The film had a great soundtrack.\"\n",
    "    ],\n",
    "    'labels': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'text': [\n",
    "        \"The movie exceeded my expectations.\",\n",
    "        \"I found the film to be disappointing.\",\n",
    "        \"The special effects were impressive.\",\n",
    "        \"The movie didn't live up to the hype.\",\n",
    "        \"I was completely engrossed in the storyline.\"\n",
    "    ],\n",
    "    'labels': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Step 2: Tokenization\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Preparing Input Features\n",
    "\n",
    "def tokenize_data(data):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        data,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_data['text'])\n",
    "val_encodings = tokenize_data(val_data['text'])\n",
    "test_encodings = tokenize_data(test_data['text'])\n",
    "\n",
    "train_labels = torch.tensor(train_data['labels'])\n",
    "val_labels = torch.tensor(val_data['labels'])\n",
    "test_labels = torch.tensor(test_data['labels'])\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "# Step 4: Model Architecture\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Step 5: Fine-tuning\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Train the model for multiple epochs\n",
    "for epoch in range(5):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss}\")\n",
    "\n",
    "# Step 6: Evaluation\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_accuracy = evaluate(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "# Step 1: Dataset Preparation\n",
    "\n",
    "train_data = {\n",
    "    'text': [\n",
    "        \"I loved the movie. It was amazing!\",\n",
    "        \"The acting was superb.\",\n",
    "        \"The plot twist at the end caught me by surprise.\",\n",
    "        \"I would highly recommend this film.\",\n",
    "        \"The movie was a bit slow in the beginning, but it picked up later.\"\n",
    "    ],\n",
    "    'labels': [1, 1, 1, 1, 0]  # 1 represents positive sentiment, 0 represents negative sentiment\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'text': [\n",
    "        \"The cinematography was excellent.\",\n",
    "        \"I didn't enjoy the movie. It was boring.\",\n",
    "        \"The characters were well-developed.\",\n",
    "        \"The story lacked depth.\",\n",
    "        \"The film had a great soundtrack.\"\n",
    "    ],\n",
    "    'labels': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'text': [\n",
    "        \"The movie exceeded my expectations.\",\n",
    "        \"I found the film to be disappointing.\",\n",
    "        \"The special effects were impressive.\",\n",
    "        \"The movie didn't live up to the hype.\",\n",
    "        \"I was completely engrossed in the storyline.\"\n",
    "    ],\n",
    "    'labels': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Step 2: Tokenization\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Preparing Input Features\n",
    "\n",
    "def tokenize_data(data):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        data,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_data['text'])\n",
    "val_encodings = tokenize_data(val_data['text'])\n",
    "test_encodings = tokenize_data(test_data['text'])\n",
    "\n",
    "train_labels = torch.tensor(train_data['labels'])\n",
    "val_labels = torch.tensor(val_data['labels'])\n",
    "test_labels = torch.tensor(test_data['labels'])\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "# Step 4: Model Architecture\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Step 5: Fine-tuning\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Train the model for multiple epochs\n",
    "for epoch in range(5):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch+1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d87bce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TinyBertForSequenceClassification' from 'transformers' (C:\\Users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23696\\7497618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTinyBertForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTinyBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Step 1: Dataset Preparation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TinyBertForSequenceClassification' from 'transformers' (C:\\Users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import TinyBertForSequenceClassification, TinyBertTokenizer, AdamW\n",
    "\n",
    "# Step 1: Dataset Preparation\n",
    "\n",
    "train_data = {\n",
    "    'text': [\n",
    "        \"I loved the movie. It was amazing!\",\n",
    "        \"The acting was superb.\",\n",
    "        \"The plot twist at the end caught me by surprise.\",\n",
    "        \"I would highly recommend this film.\",\n",
    "        \"The movie was a bit slow in the beginning, but it picked up later.\"\n",
    "    ],\n",
    "    'labels': [1, 1, 1, 1, 0]  # 1 represents positive sentiment, 0 represents negative sentiment\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'text': [\n",
    "        \"The cinematography was excellent.\",\n",
    "        \"I didn't enjoy the movie. It was boring.\",\n",
    "        \"The characters were well-developed.\",\n",
    "        \"The story lacked depth.\",\n",
    "        \"The film had a great soundtrack.\"\n",
    "    ],\n",
    "    'labels': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'text': [\n",
    "        \"The movie exceeded my expectations.\",\n",
    "        \"I found the film to be disappointing.\",\n",
    "        \"The special effects were impressive.\",\n",
    "        \"The movie didn't live up to the hype.\",\n",
    "        \"I was completely engrossed in the storyline.\"\n",
    "    ],\n",
    "    'labels': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Step 2: Tokenization\n",
    "\n",
    "tokenizer = TinyBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Preparing Input Features\n",
    "\n",
    "def tokenize_data(data):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        data,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_data['text'])\n",
    "val_encodings = tokenize_data(val_data['text'])\n",
    "test_encodings = tokenize_data(test_data['text'])\n",
    "\n",
    "train_labels = torch.tensor(train_data['labels'])\n",
    "val_labels = torch.tensor(val_data['labels'])\n",
    "test_labels = torch.tensor(test_data['labels'])\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "# Step 4: Model Architecture\n",
    "\n",
    "model = TinyBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Step 5: Fine-tuning\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Train the model for multiple epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c99f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "email_text = \"Morning BSC,_x000D_\\n_x000D_\\n _x000D_\\n_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\\n_x000D_\\n _x0 00D_\\n_x000D_\\n_x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nhttps://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx? RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D <https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx?RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlv d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\n _ x000D_\\n_x000D_\\nThank you,_x00610_\\n_x000D_\\n _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nNicole Logtenberg 1They/Theml Client Sery ice Associate_x000D_\\n   x000D _ \\n _x000D \\n _x000D \\n_x000D_\\n_x000D_\\nScotiabank 1 Commercial Banking Distribution_x000D_\\n_x000D_\\n2 Robert Speck Parkway, Mississauga Executive Centre, 4th Fl oor _x000D_\\n_x000D_\\nflississauga, ON L4Z1H8 Canada _x000D_\\n_x000D_\\nT n/a M n/a_x000D_\\n_x000D_\\nnicole.logtenberg@scoti abank.com <mailto:nicole.logtenberg@scotiabank.com> _x000D_\\n_x000D_\\nwww.scotiabank.com <https://owa.scotiamail.bns/owa/red ir.aspx?C=E1319A6qpB0GBkyogT8EbXYh6hUfHwNEIgLPPQJrM2FTEKz8rWLn3DnWezOo2s6MYDuciDE53hilSg.8tURL=file%3a%2f%2f%2fC%3a%2fUsers%2fs4 774757%2fAppData%2-fRoaming%2-Ftlicroso-Ft%2fWord%2fwww. scotiabank. corn> _x000D_\\n_x000D_\\nScotiabank is a business name used by The Bank of Nova Scotia_x000D_\\n_x000D_\\n _x000D_\\n\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern = r\"Morning BSC,_x000D_\\n_x000D_\\n _x000D_\\n_x000D_An(.*?)from Co mmercial to small business\\.(.*?)Please see SDR below for authorization\\._x000D_\\n_x000D_\\n _x0 00D_\\n_x000D_\\n_x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\n(.*?)<.*?> _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\n _ x000D_\\n_x000D_\\nThank you,_x00610_\\n_x000D_\\n _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\n(.*?)1They/Theml Client Sery ice Associate_x000D_\\n   x000D _ \\n _x000D \\n(.*?)1 Commercial Banking Distribution_x000D_\\n_x000D_\\n2 Robert Speck Parkway, Mississauga Executive Centre, 4th Fl oor _x000D_\\n_x000D_\\nflississauga, ON L4Z1H8 Canada _x000D_\\n_x000D_\\nT n/a M n/a_x000D_\\n_x000D_\\n(.*?)@scoti abank\\.com <mailto:(.*?)> _x000D_\\n_x000D_\\n(.*?)www\\.scotiabank\\.com <.*?> _x000D_\\n_x000D_\\nScotiabank is a business name used by The Bank of Nova Scotia_x000D_\\n_x000D_\\n _x000D_\\n\"\n",
    "\n",
    "# Clean the email text\n",
    "cleaned_text = re.sub(pattern, \"\", email_text)\n",
    "\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db52143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning BSC,_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\n",
      "_x000D_\n",
      " _x0 00D_\n",
      "_x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      " RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D < d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> _x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      " _ x000D_\n",
      "_x000D_\n",
      "Thank you,_x00610_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      "Nicole Logtenberg 1They/Theml Client Sery ice Associate_x000D_\n",
      "   x000D _ \n",
      " _x000D \n",
      " _x000D \n",
      "_x000D_\n",
      "_x000D_\n",
      "Scotiabank 1 Commercial Banking Distribution_x000D_\n",
      "_x000D_\n",
      "2 Robert Speck Parkway, Mississauga Executive Centre, 4th Fl oor _x000D_\n",
      "_x000D_\n",
      "flississauga, ON L4Z1H8 Canada _x000D_\n",
      "_x000D_\n",
      "T n/a M n/a_x000D_\n",
      "_x000D_\n",
      " abank.com  _x000D_\n",
      "_x000D_\n",
      " < ir.aspx?C=E1319A6qpB0GBkyogT8EbXYh6hUfHwNEIgLPPQJrM2FTEKz8rWLn3DnWezOo2s6MYDuciDE53hilSg.8tURL=file%3a%2f%2f%2fC%3a%2fUsers%2fs4 774757%2fAppData%2-fRoaming%2-Ftlicroso-Ft%2fWord%2fwww. scotiabank. corn> _x000D_\n",
      "_x000D_\n",
      "Scotiabank is a business name used by The Bank of Nova Scotia_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_email_text(email_text):\n",
    "    # Remove email addresses\n",
    "    email_text = re.sub(r'\\S+@\\S+', '', email_text)\n",
    "    \n",
    "    # Remove website addresses\n",
    "    email_text = re.sub(r'http\\S+|www\\.\\S+', '', email_text)\n",
    "    \n",
    "    return email_text\n",
    "\n",
    "# Example usage\n",
    "email_text = \"Morning BSC,_x000D_\\n_x000D_\\n _x000D_\\n_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\\n_x000D_\\n _x0 00D_\\n_x000D_\\n_x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nhttps://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx? RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D <https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx?RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlv d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\n _ x000D_\\n_x000D_\\nThank you,_x00610_\\n_x000D_\\n _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nNicole Logtenberg 1They/Theml Client Sery ice Associate_x000D_\\n   x000D _ \\n _x000D \\n _x000D \\n_x000D_\\n_x000D_\\nScotiabank 1 Commercial Banking Distribution_x000D_\\n_x000D_\\n2 Robert Speck Parkway, Mississauga Executive Centre, 4th Fl oor _x000D_\\n_x000D_\\nflississauga, ON L4Z1H8 Canada _x000D_\\n_x000D_\\nT n/a M n/a_x000D_\\n_x000D_\\nnicole.logtenberg@scoti abank.com <mailto:nicole.logtenberg@scotiabank.com> _x000D_\\n_x000D_\\nwww.scotiabank.com <https://owa.scotiamail.bns/owa/red ir.aspx?C=E1319A6qpB0GBkyogT8EbXYh6hUfHwNEIgLPPQJrM2FTEKz8rWLn3DnWezOo2s6MYDuciDE53hilSg.8tURL=file%3a%2f%2f%2fC%3a%2fUsers%2fs4 774757%2fAppData%2-fRoaming%2-Ftlicroso-Ft%2fWord%2fwww. scotiabank. corn> _x000D_\\n_x000D_\\nScotiabank is a business name used by The Bank of Nova Scotia_x000D_\\n_x000D_\\n _x000D_\\n\"\n",
    "\n",
    "\n",
    "cleaned_text = clean_email_text(email_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f9de97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Morning BSC,_x000D_\\n_x000D_\\n _x000D_\\n_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\\n_x000D_\\n _x0 00D_\\n_x000D_\\n_x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nhttps://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx? RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D <https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx?RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlv d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\n _ x000D_\\n_x000D_\\nThank you,_x00610_\\n_x000D_\\n _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nNicole Logtenberg 1They/Theml Client Sery ice Associate_x000D_\\n   x000D _ \\n _x000D \\n _x000D \\n_x000D_\\n_x000D_\\nScotiabank 1 Commercial Banking Distribution_x000D_\\n_x000D_\\n2 Robert Speck Parkway, Mississauga Executive Centre, 4th Fl oor _x000D_\\n_x000D_\\nflississauga, ON L4Z1H8 Canada _x000D_\\n_x000D_\\nT n/a M n/a_x000D_\\n_x000D_\\nnicole.logtenberg@scoti abank.com <mailto:nicole.logtenberg@scotiabank.com> _x000D_\\n_x000D_\\nwww.scotiabank.com <https://owa.scotiamail.bns/owa/red ir.aspx?C=E1319A6qpB0GBkyogT8EbXYh6hUfHwNEIgLPPQJrM2FTEKz8rWLn3DnWezOo2s6MYDuciDE53hilSg.8tURL=file%3a%2f%2f%2fC%3a%2fUsers%2fs4 774757%2fAppData%2-fRoaming%2-Ftlicroso-Ft%2fWord%2fwww. scotiabank. corn> _x000D_\\n_x000D_\\nScotiabank is a business name used by The Bank of Nova Scotia_x000D_\\n_x000D_\\n _x000D_\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_email(email_text):\n",
    "    # Remove the pattern '_x000D_\\n' and its variations\n",
    "    cleaned_text = re.sub(r'(_x000D_\\\\n\\s*)+', '', email_text)\n",
    "    \n",
    "    # Remove any remaining '_x000D_' occurrences\n",
    "    cleaned_text = cleaned_text.replace('_x000D_', '')\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "email_text = \"Morning BSC,_x000D_\\n_x000D_\\n _x000D_\\n_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\\n_x000D_\\n _x0 00D_\\n_x000D_\\n_x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nhttps://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx? RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D <https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx?RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlv d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> _x000D_\\n_x000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607a9c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning BSC,\n",
      "\n",
      " \n",
      "AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\n",
      "\n",
      " _x0 00D_\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx? RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D <https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx?RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlv d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> \n",
      "\n",
      " \n",
      "\n",
      " _ x000D_\n",
      "\n",
      "Thank you,_x00610_\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Nicole Logtenberg 1They/Theml Client Sery ice Associate\n",
      "   x000D _ \n",
      " _x000D \n",
      " _x000D \n",
      "\n",
      "\n",
      "Scotiabank 1 Commercial Banking Distribution\n",
      "\n",
      "2 Robert Speck Parkway, Mississauga Executive Centre, 4th Fl oor \n",
      "\n",
      "flississauga, ON L4Z1H8 Canada \n",
      "\n",
      "T n/a M n/a\n",
      "\n",
      "nicole.logtenberg@scoti abank.com <mailto:nicole.logtenberg@scotiabank.com> \n",
      "\n",
      "www.scotiabank.com <https://owa.scotiamail.bns/owa/red ir.aspx?C=E1319A6qpB0GBkyogT8EbXYh6hUfHwNEIgLPPQJrM2FTEKz8rWLn3DnWezOo2s6MYDuciDE53hilSg.8tURL=file%3a%2f%2f%2fC%3a%2fUsers%2fs4 774757%2fAppData%2-fRoaming%2-Ftlicroso-Ft%2fWord%2fwww. scotiabank. corn> \n",
      "\n",
      "Scotiabank is a business name used by The Bank of Nova Scotia\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_email(email_text):\n",
    "    # Remove the pattern '_x000D_\\n' and its variations\n",
    "    cleaned_text = re.sub(r'(_x000D_\\\\n\\s*)+', '', email_text)\n",
    "    \n",
    "    # Remove any remaining '_x000D_' occurrences\n",
    "    cleaned_text = cleaned_text.replace('_x000D_', '')\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "email_text = \"Morning BSC,_x000D_\\n_x000D_\\n _x000D_\\n_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\\n_x000D_\\n _x0 00D_\\n_x000D_\\n_x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nhttps://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx? RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D <https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx?RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlv d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\n _ x000D_\\n_x000D_\\nThank you,_x00610_\\n_x000D_\\n _x000D_\\n_x000D_\\n _x000D_\\n_x000D_\\nNicole Logtenberg 1They/Theml Client Sery ice Associate_x000D_\\n   x000D _ \\n _x000D \\n _x000D \\n_x000D_\\n_x000D_\\nScotiabank 1 Commercial Banking Distribution_x000D_\\n_x000D_\\n2 Robert Speck Parkway, Mississauga Executive Centre, 4th Fl oor _x000D_\\n_x000D_\\nflississauga, ON L4Z1H8 Canada _x000D_\\n_x000D_\\nT n/a M n/a_x000D_\\n_x000D_\\nnicole.logtenberg@scoti abank.com <mailto:nicole.logtenberg@scotiabank.com> _x000D_\\n_x000D_\\nwww.scotiabank.com <https://owa.scotiamail.bns/owa/red ir.aspx?C=E1319A6qpB0GBkyogT8EbXYh6hUfHwNEIgLPPQJrM2FTEKz8rWLn3DnWezOo2s6MYDuciDE53hilSg.8tURL=file%3a%2f%2f%2fC%3a%2fUsers%2fs4 774757%2fAppData%2-fRoaming%2-Ftlicroso-Ft%2fWord%2fwww. scotiabank. corn> _x000D_\\n_x000D_\\nScotiabank is a business name used by The Bank of Nova Scotia_x000D_\\n_x000D_\\n _x000D_\\n\"\n",
    "\n",
    "cleaned_email = clean_email(email_text)\n",
    "print(cleaned_email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "180f67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning BSC,_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\n",
      "_x000D_\n",
      " _x0 00D_\n",
      "_x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      " _ x000D_\n",
      "_x000D_\n",
      "Thank you\n"
     ]
    }
   ],
   "source": [
    "def remove_lines_with_percentage(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines_without_percentage = [line for line in lines if \"%\" not in line]\n",
    "    cleaned_text = \"\\n\".join(lines_without_percentage)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "cleaned_text = remove_lines_with_percentage(remove_text_after_thank_you(email_text))\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef34fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning BSC,_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_AnCan you please update the customer type for BLVD Construction Inc. from Co mmercial to small business. This was authorized in March 2021. Please see SDR below for authorization. x000D_\n",
      "_x000D_\n",
      " _x0 00D_\n",
      "_x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      "https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx? RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlvd%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20D=als%2FTransfer7.20t o%20Small%20Business&FolderCTID=Ox0120003A06EA04D6EED8469261EACEBO8C83FE&View=%7B55641070%2D726F%2D4881%2DAFBD%2D4C1A19B6613 8%7D <https://scotiabank.sharepoint.com/sites/CBSDR21/tor/Forms/AllItems.aspx?RootFolder=%2Fsites%2FCBSDR21%2Ftor%2FB%2FBlv d%20Construction%20Inc%2F1%2DAnnual%20Review%20and%20New%20Deals%2FTransfer%20to%20Small%20Business&FolderCTID=0x0120003A06E A04D6EBD8469261EACEBO8C83FE&View7%7B55B41070%2D726F%2D4881%2DAFBD%2D4C1A19B66138%7D> _x000D_\n",
      "_x000D_\n",
      " _x000D_\n",
      "_x000D_\n",
      " _ x000D_\n",
      "_x000D_\n",
      "Thank you\n"
     ]
    }
   ],
   "source": [
    "def remove_text_after_thank_you(text):\n",
    "    patterns = [\"thank you\", \"thanks\"]\n",
    "    for pattern in patterns:\n",
    "        index = text.lower().find(pattern)\n",
    "        if index != -1:\n",
    "            text = text[:index + len(pattern)]\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "#email_text = \"Morning BSC, ... Thank you for your assistance. Have a great day!\"\n",
    "\n",
    "cleaned_text = remove_text_after_thank_you(email_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda029dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['orange', 'juice', 'fruit', 'banana', 'apple', 'pie']\n",
      "Topic 2: ['banana', 'apple', 'pie', 'fruit', 'juice', 'orange']\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "def perform_tf_idf_lda_topic_modeling(documents, num_topics):\n",
    "    # Create a dictionary from the documents\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    \n",
    "    # Create a corpus (vector representation of the documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Compute TF-IDF weights for the corpus\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    # Build the LDA model using TF-IDF weights\n",
    "    lda_model = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    # Get the topics and their keywords\n",
    "    topics = []\n",
    "    for topic_id, topic in lda_model.print_topics(num_topics=num_topics):\n",
    "        keywords = topic.split('\"')[1::2]\n",
    "        topics.append(keywords)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    [\"apple\", \"banana\", \"fruit\", \"juice\"],\n",
    "    [\"orange\", \"fruit\", \"juice\"],\n",
    "    [\"apple\", \"fruit\", \"pie\"],\n",
    "    [\"banana\", \"fruit\"],\n",
    "    [\"orange\", \"juice\"]\n",
    "]\n",
    "\n",
    "num_topics = 2\n",
    "\n",
    "topics = perform_tf_idf_lda_topic_modeling(documents, num_topics)\n",
    "for topic_id, keywords in enumerate(topics):\n",
    "    print(f\"Topic {topic_id+1}: {keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5849d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\vikra\\anaconda3\\envs\\new\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Installing collected packages: Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.34\n",
      "    Uninstalling Cython-0.29.34:\n",
      "      Successfully uninstalled Cython-0.29.34\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61019cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Topic 1 - Probability: 0.8138322234153748\n",
      "Topic 2 - Probability: 0.18616776168346405\n",
      "\n",
      "Document 2:\n",
      "Topic 1 - Probability: 0.8543345332145691\n",
      "Topic 2 - Probability: 0.14566552639007568\n",
      "\n",
      "Document 3:\n",
      "Topic 1 - Probability: 0.8840864300727844\n",
      "Topic 2 - Probability: 0.11591357737779617\n",
      "\n",
      "Document 4:\n",
      "Topic 1 - Probability: 0.8526530265808105\n",
      "Topic 2 - Probability: 0.14734697341918945\n",
      "\n",
      "Document 5:\n",
      "Topic 1 - Probability: 0.176798015832901\n",
      "Topic 2 - Probability: 0.8232020139694214\n",
      "\n",
      "Document 6:\n",
      "Topic 1 - Probability: 0.8209992051124573\n",
      "Topic 2 - Probability: 0.17900077998638153\n",
      "\n",
      "Document 7:\n",
      "Topic 1 - Probability: 0.14182975888252258\n",
      "Topic 2 - Probability: 0.8581702709197998\n",
      "\n",
      "Document 8:\n",
      "Topic 1 - Probability: 0.13258852064609528\n",
      "Topic 2 - Probability: 0.8674114942550659\n",
      "\n",
      "Document 9:\n",
      "Topic 1 - Probability: 0.8889032602310181\n",
      "Topic 2 - Probability: 0.11109672486782074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def perform_tf_idf_lda_topic_modeling(documents, num_topics):\n",
    "    # Preprocess the documents\n",
    "    processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "    \n",
    "    # Create a dictionary from the processed documents\n",
    "    dictionary = corpora.Dictionary(processed_documents)\n",
    "    \n",
    "    # Create a corpus (vector representation of the documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "    \n",
    "    # Compute TF-IDF weights for the corpus\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    # Build the LDA model using TF-IDF weights\n",
    "    lda_model = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    # Assign topics to each document\n",
    "    document_topics = []\n",
    "    for doc_bow in corpus:\n",
    "        topic_dist = lda_model.get_document_topics(doc_bow)\n",
    "        document_topics.append(topic_dist)\n",
    "    \n",
    "    return document_topics\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    \"I love to play soccer\",\n",
    "    \"Soccer is a popular sport\",\n",
    "    \"I enjoy watching soccer matches\",\n",
    "    \"Basketball is also a great sport\",\n",
    "    \"I prefer soccer over basketball\",\n",
    "    \"Music is my passion\",\n",
    "    \"I play the guitar and piano\",\n",
    "    \"Singing brings me joy\",\n",
    "    \"I listen to different genres of music\"\n",
    "]\n",
    "\n",
    "num_topics = 2\n",
    "\n",
    "document_topics = perform_tf_idf_lda_topic_modeling(documents, num_topics)\n",
    "\n",
    "# Print topics for each document\n",
    "for doc_id, topics in enumerate(document_topics):\n",
    "    print(f\"Document {doc_id+1}:\")\n",
    "    for topic, prob in topics:\n",
    "        print(f\"Topic {topic+1} - Probability: {prob}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19692901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.8138322), (1, 0.18616776)],\n",
       " [(0, 0.85433453), (1, 0.14566553)],\n",
       " [(0, 0.88408643), (1, 0.11591358)],\n",
       " [(0, 0.852653), (1, 0.14734697)],\n",
       " [(0, 0.17679802), (1, 0.823202)],\n",
       " [(0, 0.8209992), (1, 0.17900078)],\n",
       " [(0, 0.14182976), (1, 0.8581703)],\n",
       " [(0, 0.13258852), (1, 0.8674115)],\n",
       " [(0, 0.88890326), (1, 0.111096725)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b972e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['play', 'passion', 'love', 'piano', 'guitar', 'music', 'brings', 'joy', 'singing', 'soccer']\n",
      "Topic 2: ['basketball', 'sport', 'soccer', 'prefer', 'popular', 'also', 'great', 'enjoy', 'watching', 'matches']\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def perform_tf_idf_lda_topic_modeling(documents, num_topics):\n",
    "    # Preprocess the documents\n",
    "    processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "    \n",
    "    # Create a dictionary from the processed documents\n",
    "    dictionary = corpora.Dictionary(processed_documents)\n",
    "    \n",
    "    # Create a corpus (vector representation of the documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "    \n",
    "    # Compute TF-IDF weights for the corpus\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    # Build the LDA model using TF-IDF weights\n",
    "    lda_model = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    # Get the topics and their keywords\n",
    "    topics = []\n",
    "    for topic_id, topic in lda_model.print_topics(num_topics=num_topics):\n",
    "        keywords = topic.split('\"')[1::2]\n",
    "        topics.append(keywords)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    \"I love to play soccer\",\n",
    "    \"Soccer is a popular sport\",\n",
    "    \"I enjoy watching soccer matches\",\n",
    "    \"Basketball is also a great sport\",\n",
    "    \"I prefer soccer over basketball\",\n",
    "    \"Music is my passion\",\n",
    "    \"I play the guitar and piano\",\n",
    "    \"Singing brings me joy\",\n",
    "    \"I listen to different genres of music\"\n",
    "]\n",
    "\n",
    "num_topics = 2\n",
    "\n",
    "topics = perform_tf_idf_lda_topic_modeling(documents, num_topics)\n",
    "for topic_id, keywords in enumerate(topics):\n",
    "    print(f\"Topic {topic_id+1}: {keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c264da4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                             text\n",
      "0   1      This is the first document.\n",
      "1   2     The second document is here.\n",
      "2   3  And this is the third document.\n",
      "3   4   Another document for analysis.\n",
      "4   5    The last document in the set.\n",
      "   id                             text  topics\n",
      "0   1      This is the first document.       2\n",
      "1   2     The second document is here.       2\n",
      "2   3  And this is the third document.       2\n",
      "3   4   Another document for analysis.       1\n",
      "4   5    The last document in the set.       3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'text': [\n",
    "        'This is the first document.',\n",
    "        'The second document is here.',\n",
    "        'And this is the third document.',\n",
    "        'Another document for analysis.',\n",
    "        'The last document in the set.'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "# Preprocess the text data\n",
    "texts = [simple_preprocess(text) for text in df['text']]\n",
    "\n",
    "# Create a dictionary from the preprocessed texts\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Create a Bag-of-Words representation for each document\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Define the number of topics for topic modeling\n",
    "num_topics = 5\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "# Get the topic distribution for each document\n",
    "topic_predictions = [lda_model[doc] for doc in corpus]\n",
    "\n",
    "# Function to get the most probable topic for a document\n",
    "def get_most_probable_topic(topic_distribution):\n",
    "    return max(topic_distribution, key=lambda item: item[1])[0]\n",
    "\n",
    "# Add a new column to the DataFrame with the assigned topics\n",
    "df['topics'] = [get_most_probable_topic(topic_dist) for topic_dist in topic_predictions]\n",
    "\n",
    "# Print the DataFrame with assigned topics\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712cc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
